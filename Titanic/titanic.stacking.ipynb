{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Deprecation and Incorrect Usage Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import time\n",
    "import gc\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "import gc\n",
    "\n",
    "from sklearn.linear_model import Lasso, ElasticNet, RANSACRegressor, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import LinearSVR,SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "### seaborn에서 한글 나오게하기\n",
    "sns.set(font=\"New Gulim\")\n",
    "\n",
    "import zipfile\n",
    "import stacking\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = './data'\n",
    "train_origin = pd.read_csv('{}/train.csv'.format(INPUT_FOLDER))\n",
    "test_origin = pd.read_csv('{}/test.csv'.format(INPUT_FOLDER))\n",
    "print(train_origin.shape, test_origin.shape)\n",
    "\n",
    "test_origin['Survived'] = 0\n",
    "test_id = test_origin['PassengerId'][0]\n",
    "print(test_id)\n",
    "\n",
    "test_origin['Survived'] = 0\n",
    "test_id = test_origin['PassengerId'][0]\n",
    "print(test_id)\n",
    "\n",
    "data = pd.concat([train_origin, test_origin])\n",
    "print(data.shape)\n",
    "\n",
    "data['Title'] = data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "major_titles = data['Title'].value_counts()[:4].index.tolist()\n",
    "\n",
    "map_title_misc = {'Don': 'Mrs', 'Dona': 'Mrs',  'Capt': 'Mr', 'Col': 'Mr', 'Jonkheer': 'Mr',\n",
    "                  'Lady': 'Mrs', 'Major': 'Mr', 'Mlle': 'Miss', 'Mme': 'Mrs', 'Rev': 'Mrs', 'Rev': 'Mrs',\n",
    "                  'Sir' : 'Mr', 'the Countess': 'Mrs', 'Ms': 'Miss'}\n",
    "\n",
    "def clean_title(row):\n",
    "    title = row['Title']\n",
    "    if title in map_title_misc.keys():\n",
    "        row['Title'] = map_title_misc[title]\n",
    "    elif title == 'Dr':\n",
    "        sex = row['Sex']\n",
    "        row['Title'] = 'Mr' if sex == 'mail' else 'Mrs'\n",
    "    return row\n",
    "\n",
    "data = data.apply(clean_title, axis=1)\n",
    "\n",
    "def cabin_class(row):\n",
    "    cabin = str(row['Cabin'])\n",
    "    row['CabinClass'] = np.nan\n",
    "    if cabin is not None:\n",
    "        row['CabinClass'] = cabin[0]\n",
    "    return row\n",
    "\n",
    "data = data.apply(cabin_class, axis=1)\n",
    "\n",
    "data.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
    "\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\n",
    "data['Fare'].fillna(data['Fare'].median(), inplace = True)\n",
    "data['Age'].fillna(data['Age'].median(), inplace = True)\n",
    "\n",
    "categorical_features = ['Sex', 'Title', 'Embarked']\n",
    "def category_to_code(df, categorical_features):\n",
    "    df_copy = df.copy()\n",
    "    for feature in categorical_features:\n",
    "        df_copy[feature] = df_copy[feature].astype('category').cat.codes\n",
    "    return df_copy\n",
    "\n",
    "data_code = category_to_code(data, categorical_features)\n",
    "data_code.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummy = pd.get_dummies(data)\n",
    "data_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_dummy.loc[data_dummy['PassengerId'] < test_id]\n",
    "df.drop(columns='PassengerId', inplace=True)\n",
    "\n",
    "test = data_dummy.loc[data_dummy['PassengerId'] >= test_id]\n",
    "test.drop(columns=['PassengerId', 'Survived'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df['Survived']\n",
    "x_train = df.drop(columns=['Survived'])\n",
    "x_test = test\n",
    "print(x_train.shape, y_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "reload(stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "et_params = {\n",
    "    'criterion':'gini', 'max_leaf_nodes':30, 'n_estimators':1000, 'min_impurity_split':0.0000001,\n",
    "    'max_features':0.6, 'max_depth':10, 'min_samples_leaf':20, 'min_samples_split':2,\n",
    "    'min_weight_fraction_leaf':0.0, 'bootstrap':True,\n",
    "    'random_state':1, 'verbose':False }\n",
    "\n",
    "et_model = stacking.SklearnWrapper(clf = ExtraTreesClassifier, params=et_params)\n",
    "\n",
    "et_train, et_test = stacking.get_oof(et_model, x_train, y_train, x_test, accuracy_score, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params1 = {\n",
    "    'boosting':'gbdt', 'num_leaves':10, 'learning_rate':0.01, 'min_sum_hessian_in_leaf':0.1,\n",
    "    'max_depth':4, 'feature_fraction':0.5, 'min_data_in_leaf':4, 'poission_max_delta_step':0.7,\n",
    "    'bagging_fraction':0.8, 'min_gain_to_split':0, 'scale_pos_weight':1.0,\n",
    "    'lambda_l2':0.1, 'lambda_l1':0.1, 'huber_delta':1.0, 'bagging_freq':1,\n",
    "    'objective':'binary', 'seed':1, 'categorical_feature':0, 'xgboost_dart_mode':False,\n",
    "    'drop_rate':0.1, 'skip_drop':0.5, 'max_drop':50, 'top_rate':0.1, 'other_rate':0.1,\n",
    "    'max_bin':255, 'min_data_in_bin':50, 'bin_construct_sample_cnt':1000000,\n",
    "    'two_round':False, 'uniform_drop':False,'metric': 'binary_logloss','threads':6\n",
    "}\n",
    "\n",
    "lgbm_params2 = {\n",
    "    'boosting':'gbdt', 'num_leaves':24,'learning_rate':0.03, 'min_sum_hessian_in_leaf':0.1,\n",
    "    'max_depth':6, 'feature_fraction':0.5, 'min_data_in_leaf':50, 'poission_max_delta_step':0.7, 'bagging_fraction':0.8,\n",
    "    'min_gain_to_split':0, 'scale_pos_weight':1.0, 'lambda_l2':0.1, 'lambda_l1':0.1, 'huber_delta':0.05,\n",
    "    'bagging_freq':1, 'objective':'binary', 'seed':1, 'categorical_feature':0 ,'xgboost_dart_mode':False, 'drop_rate':0.1,\n",
    "    'skip_drop':0.5, 'max_drop':50, 'top_rate':0.1, 'other_rate':0.1, 'max_bin':255, 'min_data_in_bin':50,\n",
    "    'bin_construct_sample_cnt':1000000, 'two_round':False, 'uniform_drop':False,'metric': 'binary_logloss','threads':6\n",
    "}\n",
    "\n",
    "lgbm_params3 = {\n",
    "    'boosting':'gbdt', 'num_leaves':28, 'learning_rate':0.03, 'min_sum_hessian_in_leaf':0.1, 'max_depth':7,\n",
    "    'feature_fraction':0.6, 'min_data_in_leaf':70, 'poission_max_delta_step':0.7, 'bagging_fraction':0.8,\n",
    "    'min_gain_to_split':0, 'scale_pos_weight':1.0, 'lambda_l2':0.1, 'lambda_l1':0.1, 'fair_c':0.01, 'bagging_freq':1,\n",
    "    'objective':'binary', 'seed':1, 'categorical_feature':0, 'xgboost_dart_mode':False, 'drop_rate':0.1, 'skip_drop':0.5,\n",
    "    'max_drop':50, 'top_rate':0.1, 'other_rate':0.1, 'max_bin':255, 'min_data_in_bin':50, 'bin_construct_sample_cnt':1000000,\n",
    "    'two_round':False, 'uniform_drop':False,'metric': 'binary_logloss','threads':6\n",
    "}\n",
    "\n",
    "lgbm_params4 = {\n",
    "    'boosting':'gbdt', 'num_leaves':16, 'learning_rate':0.003, 'min_sum_hessian_in_leaf':0.1, 'max_depth':7,\n",
    "    'feature_fraction':0.5, 'min_data_in_leaf':70, 'poission_max_delta_step':0.7, 'bagging_fraction':0.8, \n",
    "    'min_gain_to_split':0, 'scale_pos_weight':1.0, 'lambda_l2':0.1, 'lambda_l1':0.1, 'bagging_freq':1, 'objective':'binary',\n",
    "    'seed':1, 'categorical_feature':0, 'xgboost_dart_mode':False, 'drop_rate':0.1, 'skip_drop':0.5, 'max_drop':50, \n",
    "    'top_rate':0.1,'other_rate':0.1, 'max_bin':255, 'min_data_in_bin':50, 'bin_construct_sample_cnt':1000000, \n",
    "    'two_round':False,'uniform_drop':False,'metric': 'binary_logloss','threads':6\n",
    "}\n",
    "\n",
    "xgb_params1 = {\n",
    "    'booster':'gbtree', 'objective':'binary:logistic', 'max_leaves':0, 'eta':0.02, 'gamma':1,\n",
    "    'max_depth':4, 'colsample_bylevel':1.0, 'min_child_weight':4.0, 'max_delta_step':0.0, 'subsample':0.8, \n",
    "    'colsample_bytree':0.5,'scale_pos_weight':1.0, 'alpha':1.0, 'lambda':5.0, 'seed':1\n",
    "}\n",
    "\n",
    "xgb_params2 = {\n",
    "    'booster':'gblinear', 'objective':'binary:logistic', 'max_leaves':0, 'eta':0.1,'gamma':1,\n",
    "    'max_depth':4, 'colsample_bylevel':1.0, 'min_child_weight':4.0, 'max_delta_step':0.0, 'subsample':0.8, \n",
    "    'colsample_bytree':0.5,'scale_pos_weight':1.0, 'alpha':10.0, 'lambda':1.0, 'seed':1\n",
    "}\n",
    "\n",
    "sgd_param = {\n",
    "    'loss':'huber','penalty':'l2','alpha':1,'l1_ratio':0.15,'eta0':0.001,\n",
    "    'fit_intercept':True,'shuffle':True,'random_state':1,\n",
    "}\n",
    "\n",
    "gbm_param = {\n",
    "    'n_estimators' :100, 'learning_rate':0.1, 'min_samples_split' :0.00001,\n",
    "    'subsample':1.0, 'max_depth':5, 'max_features':0.4,\n",
    "    'min_samples_leaf' :0.5, 'random_state' :1\n",
    "}\n",
    "\n",
    "lasso_params={\n",
    "    'alpha':0.003,\n",
    "    'normalize':True,\n",
    "    'max_iter':200,'fit_intercept':True,'tol':0.007,\n",
    "    'warm_start':True\n",
    "}\n",
    "\n",
    "ridge_params={\n",
    "    'alpha':0.2,\n",
    "    'normalize':True,\n",
    "    'max_iter':200,'fit_intercept':False,'solver':'auto'\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'criterion':'gini', 'max_leaf_nodes':30, 'n_estimators':1000, 'min_impurity_split':0.0000001,\n",
    "    'max_features':0.25, 'max_depth':6, 'min_samples_leaf':20, 'min_samples_split':2,\n",
    "    'min_weight_fraction_leaf':0.0, 'bootstrap':True,\n",
    "    'random_state':1, 'verbose':False\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = stacking.KerasWrapper(ExtraTreesClassifier, None, params=rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model1 = stacking.LgbmWrapper(params=lgbm_params1, num_rounds = 1500, ealry_stopping=100,\n",
    "                                   verbose_eval=False, base_score=True, maximize=False,\n",
    "                                   y_value_log=False)\n",
    "\n",
    "lgbm_model2 = stacking.LgbmWrapper(params=lgbm_params2, num_rounds = 1500, ealry_stopping=100,\n",
    "                                   verbose_eval=False, base_score=True, maximize=False,\n",
    "                                   y_value_log=False)\n",
    "\n",
    "lgbm_model3 = stacking.LgbmWrapper(params=lgbm_params3, num_rounds = 1500, ealry_stopping=100,\n",
    "                                   verbose_eval=False, base_score=True, maximize=False,\n",
    "                                   y_value_log=False)\n",
    "\n",
    "lgbm_model4 = stacking.LgbmWrapper(params=lgbm_params4, num_rounds = 1500, ealry_stopping=100,\n",
    "                                   verbose_eval=False, base_score=True, maximize=False,\n",
    "                                   y_value_log=False)\n",
    "\n",
    "xgb_model1 = stacking.XgbWrapper(params=xgb_params1, num_rounds = 1500, ealry_stopping=100,\n",
    "                                   verbose_eval=False, base_score=True, maximize=False,\n",
    "                                   y_value_log=False)\n",
    "\n",
    "xgb_model2 = stacking.XgbWrapper(params=xgb_params2, num_rounds = 1500, ealry_stopping=100,\n",
    "                                   verbose_eval=False, base_score=True, maximize=False,\n",
    "                                   y_value_log=False)\n",
    "\n",
    "gbm_model = stacking.SklearnWrapper(clf = GradientBoostingClassifier,params=gbm_param)\n",
    "\n",
    "ridge_model = stacking.SklearnWrapper(clf = Ridge,params=ridge_params)\n",
    "\n",
    "lasso_model = stacking.SklearnWrapper(clf = Lasso,params=lasso_params)\n",
    "\n",
    "rf_model = stacking.SklearnWrapper(clf = RandomForestClassifier,params=rf_params)\n",
    "\n",
    "et_model = stacking.SklearnWrapper(clf = ExtraTreesClassifier,params=rf_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgbm1_train,lgbm1_test = stacking.get_oof(lgbm_model1,x_train,y_train,x_test, log_loss, NFOLDS=5)\n",
    "lgbm2_train,lgbm2_test = stacking.get_oof(lgbm_model2,x_train,y_train,x_test, log_loss, NFOLDS=5)\n",
    "lgbm3_train,lgbm3_test = stacking.get_oof(lgbm_model3,x_train,y_train,x_test, log_loss, NFOLDS=5)\n",
    "lgbm4_train,lgbm4_test = stacking.get_oof(lgbm_model4,x_train,y_train,x_test, log_loss, NFOLDS=5)\n",
    "gc.collect()\n",
    "xgb1_train,xgb1_test = stacking.get_oof(xgb_model1,x_train,y_train,x_test, log_loss, NFOLDS=5)\n",
    "xgb2_train,xgb2_test = stacking.get_oof(xgb_model2,x_train,y_train,x_test, log_loss, NFOLDS=5)\n",
    "gc.collect()\n",
    "gbm1_train,gbm1_test = stacking.get_oof(gbm_model,x_train.fillna(-1),y_train,x_test.fillna(-1), log_loss,NFOLDS=5)\n",
    "ridge_train,ridge_test = stacking.get_oof(ridge_model,x_train.fillna(-1),y_train,x_test.fillna(-1), log_loss,NFOLDS=5)\n",
    "lasso_train,lasso_test = stacking.get_oof(lasso_model,x_train.fillna(-1),y_train,x_test.fillna(-1), log_loss,NFOLDS=5)\n",
    "rf_train,rf_test = stacking.get_oof(rf_model,x_train.fillna(-1),y_train,x_test.fillna(-1), log_loss,NFOLDS=5)\n",
    "et_train,et_test = stacking.get_oof(et_model,x_train.fillna(-1),y_train,x_test.fillna(-1), log_loss,NFOLDS=5)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_second_layer = np.concatenate((lgbm1_train, lgbm2_train, lgbm3_train,\n",
    "                         lgbm4_train, xgb1_train, xgb2_train,\n",
    "                         gbm1_train,ridge_train, lasso_train, rf_train,et_train), axis=1)\n",
    "x_test_second_layer = np.concatenate((lgbm1_test, lgbm2_test, lgbm3_test,\n",
    "                        lgbm4_test,xgb1_test,xgb2_test,\n",
    "                       gbm1_test,ridge_test,lasso_test,rf_test,et_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_ex_no = 9\n",
    "lgbm_meta_params = {\n",
    "    'boosting':'gbdt', 'num_leaves':28, 'learning_rate':0.03, 'min_sum_hessian_in_leaf':0.1,\n",
    "    'max_depth':7, 'feature_fraction':0.6, 'min_data_in_leaf':70, 'poission_max_delta_step':0.7,\n",
    "    'bagging_fraction':0.8, 'min_gain_to_split':0, 'scale_pos_weight':1.0,\n",
    "    'lambda_l2':0.1, 'lambda_l1':0.1, 'fair_c':1.0, 'bagging_freq':1,\n",
    "    'objective':'binary', 'seed':1, 'categorical_feature':0, 'xgboost_dart_mode':False,\n",
    "    'drop_rate':0.1, 'skip_drop':0.5, 'max_drop':50, 'top_rate':0.1, 'other_rate':0.1,\n",
    "    'max_bin':255, 'min_data_in_bin':50, 'bin_construct_sample_cnt':1000000,\n",
    "    'two_round':False, 'uniform_drop':False,'metric': 'binary_logloss','threads':6\n",
    "}\n",
    "\n",
    "lgbm_meta_model = stacking.LgbmWrapper(params=lgbm_meta_params, num_rounds = 2000, ealry_stopping=100,\n",
    "               verbose_eval=False, base_score=True, maximize=False, y_value_log=False)\n",
    "\n",
    "lgbm_cv_score,best_round = stacking.kfold_test(lgbm_meta_model, pd.DataFrame(x_train_second_layer), \n",
    "                                               y_train,  log_loss, NFOLDS=5 )\n",
    "\n",
    "\n",
    "d_train_all = lgbm.Dataset(pd.DataFrame(x_train_second_layer), label=y_train)\n",
    "bst = lgbm.train(lgbm_params4,d_train_all,best_round)\n",
    "predictions = bst.predict(pd.DataFrame(x_test_second_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fscore result\")\n",
    "fscore_df = pd.concat([pd.DataFrame(bst.feature_name()),pd.DataFrame(bst.feature_importance())],axis=1)\n",
    "fscore_df.columns = ['column','fscore']\n",
    "fscore_df['fscore'] = fscore_df['fscore'].astype(int)\n",
    "fscore_df.sort_values(by='fscore',ascending=False,inplace=True)\n",
    "fscoe_output = 'fscore\\\\ex_'+str(lgbm_ex_no)+'_lgbm_fscore_'+ str(lgbm_cv_score)+ '.csv'\n",
    "fscore_df.to_csv(fscoe_output)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "lgbm.plot_importance(bst, height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_predictions = [(0 if x < 0.5 else 1) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_dummy.loc[data_dummy['PassengerId'] >= test_id]\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test[\"PassengerId\"],\n",
    "    \"Survived\": binary_predictions\n",
    "})\n",
    "submission.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Predict\")\n",
    "# sub = pd.read_csv('input/sample_submission.csv')\n",
    "# for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "#     print(c)\n",
    "#     sub[c] = predictions\n",
    "\n",
    "# print(\"Wrting Files\")\n",
    "# sub_output = 'output\\\\ex_'+str(lgbm_ex_no)+'_lightgbm_'+str(lgbm_cv_score)\n",
    "# sub.to_csv(sub_output+'.csv', index=False, float_format='%.4f') # Thanks to @inversion\n",
    "\n",
    "# print(\"File Zip\")\n",
    "# jungle_zip = zipfile.ZipFile(sub_output +'.zip', 'w')\n",
    "# jungle_zip.write(sub_output + '.csv', compress_type=zipfile.ZIP_DEFLATED)\n",
    " \n",
    "# jungle_zip.close()\n",
    "# print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
